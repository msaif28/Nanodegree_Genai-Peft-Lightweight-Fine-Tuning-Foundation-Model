# -*- coding: utf-8 -*-
"""Lightweight-Fine-Tuning-20th.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hiKUM1aP82JoiST7ivneJFEQ1-CMVM-M

# Lightweight Fine-Tuning Project

TODO: In this cell, describe your choices for each of the following

* PEFT technique: Lora
* Model: gpt2
* Evaluation approach:  Sentiment Analysis
* Fine-tuning dataset: zeroshot/twitter-financial-news-sentiment

## Loading and Evaluating a Foundation Model

TODO: In the cells below, load your chosen pre-trained Hugging Face model and evaluate its performance prior to fine-tuning. This step includes loading an appropriate tokenizer and dataset.
"""

!pip install datasets transformers accelerate -U
!pip install peft

data = 'zeroshot/twitter-financial-news-sentiment'
mod = 'gpt2'

from datasets import load_dataset

# Data has a training and validation set, but no test set.
dataset = load_dataset(data)

# Perform the train-test split on the 'train' dataset with shuffling
split_result = dataset['train'].train_test_split(test_size=0.3, shuffle=True, seed=42)

# Update the dataset dictionary directly with the new splits
dataset.update({
    'train': split_result['train'],
    'test': split_result['test']
})

# Showing first entry for train set
dataset['train'][0]

from transformers import AutoTokenizer

# Initialize the tokenizer
tokenizer = AutoTokenizer.from_pretrained(mod)
tokenizer.pad_token = tokenizer.eos_token

# Defining a function to tokenize a batch of texts
def tokenize_batch(examples):
    return tokenizer(examples['text'], padding="max_length", truncation=True)

# Tokenize all the examples in each split of the dataset
tokenized_dataset = {}
splits = ['train', 'test', 'validation']
for split in splits:
    # Check if the split exists in the dataset to avoid KeyError
    if split in dataset:
        tokenized_dataset[split] = dataset[split].map(tokenize_batch, batched=True)

# Show the structure of the tokenized dataset
print(tokenized_dataset)

from transformers import AutoModelForSequenceClassification

# Load the pretrained model with specific configuration options
model = AutoModelForSequenceClassification.from_pretrained(
    mod,
    num_labels=3,
    id2label={0: 'Negative', 1: 'Positive', 2: 'Indifferent'},
    label2id={'Negative': 0, 'Positive': 1, 'Indifferent': 2}
)

# Update the model's tokenizer pad token id in its configuration
model.config.pad_token_id = tokenizer.eos_token_id

# Freeze the parameters in the base model to prevent them from being updated during training
# Freeze the model base parameters
for param in model.base_model.parameters():
    param.requires_grad = False

import numpy as np
from transformers import DataCollatorWithPadding, Trainer, TrainingArguments

# evaluation metrics
def compute_metrics(eval_pred):
    predictions, labels = eval_pred
    predictions = np.argmax(predictions, axis=1)
    return {"accuracy": (predictions == labels).mean()}

trainer = Trainer(
    model=model,
    args=TrainingArguments(
        output_dir="./data/train",
        # Set the learning rate
        learning_rate = 2e-5,
        # Set the per device train batch size and eval batch size
        per_device_train_batch_size = 16,
        per_device_eval_batch_size = 16,
        # Evaluate and save the model after each epoch
        evaluation_strategy="epoch",
        save_strategy="epoch",
        num_train_epochs=2,
        weight_decay=0.01,
        load_best_model_at_end=True,
    ),
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["validation"],
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.evaluate()

model.save_pretrained('gpt2-senti-analysis')

predicted = trainer.predict(tokenized_dataset['test'])
import pandas as pd
df = pd.DataFrame(
    {
        "predictions": predicted.predictions.argmax(axis=1),
        "actual": predicted.label_ids,
    }
)
df

accuracy = (df['predictions'] == df['actual']).mean()
print(f'Accuracy: {accuracy*100:.2f}%')

"""## Performing Parameter-Efficient Fine-Tuning"""



from peft import get_peft_config, get_peft_model, LoraConfig, TaskType
# lora configuration
config = LoraConfig(
    task_type=TaskType.SEQ_CLS,
    inference_mode=False,
    r=8,
    lora_alpha=32,
    lora_dropout=0.1,
    bias = 'none',
    target_modules=['c_attn', 'c_proj']
)

# load in the saved gpt2-senti-analysis model
from transformers import AutoModelForSequenceClassification
model = AutoModelForSequenceClassification.from_pretrained('gpt2-senti-analysis')

# create the lora model
lora_model = get_peft_model(model, config)
lora_model.print_trainable_parameters()

#Here we training Lora model
trainer = Trainer(
    model=lora_model,
    args=TrainingArguments(
        output_dir="./data/train-lora",
        # Set the learning rate
        learning_rate = 2e-5,
        # Set the per device train batch size and eval batch size
        per_device_train_batch_size = 16,
        per_device_eval_batch_size = 16,
        # Evaluate and save the model after each epoch
        evaluation_strategy="epoch",
        save_strategy="epoch",
        num_train_epochs=2,
        weight_decay=0.01,
        load_best_model_at_end=True,
    ),
    train_dataset=tokenized_dataset["train"],
    eval_dataset=tokenized_dataset["test"],
    tokenizer=tokenizer,
    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),
    compute_metrics=compute_metrics,
)

trainer.train()

trainer.evaluate()

lora_model.save_pretrained("gpt2-lora-model")

predicted = trainer.predict(tokenized_dataset['test'])
actual = np.array(tokenized_dataset['test']['label'])
x = np.stack((predicted.label_ids, actual))
import pandas as pd
df = pd.DataFrame(
    {
        "predictions": predicted.predictions.argmax(axis=1),
        "actual": predicted.label_ids,
    }
)
df

# Calculate accuracy
accuracy = (df['predictions'] == df['actual']).mean()
print(f'Accuracy: {accuracy*100:.2f}%')

"""## Performing Inference with a PEFT Model"""

# load in the model
from peft import AutoPeftModelForSequenceClassification
inference_model = AutoPeftModelForSequenceClassification.from_pretrained('gpt2-lora-model')
inference_model

tokenizer = AutoTokenizer.from_pretrained("gpt2")

# sentiment function
def get_sentiment(tweet):
    inputs = tokenizer(tweet, return_tensors="pt")
    logits = inference_model(**inputs).logits
    predicted_class_id = logits.argmax().item()
    print("Sentiment: ")
    return inference_model.config.id2label[predicted_class_id]

get_sentiment('The Federal Reserve has just cut interest rates by 0.25 percentage points.')

get_sentiment('The slowdown in the United States manufacturing sector is putting pressure on the broader economic stability.')

get_sentiment('Global tech companies are facing increased regulatory scrutiny across multiple countries.')

